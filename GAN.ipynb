{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNKdmtuO8Hw4un/nbeqbNdp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"brAjrwc78Kmi","executionInfo":{"status":"ok","timestamp":1741843122937,"user_tz":-330,"elapsed":5646,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"0c76f8a0-6a91-465b-9734-111f2ed07a32"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.5.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BHM4opVN71gl","executionInfo":{"status":"ok","timestamp":1741845253835,"user_tz":-330,"elapsed":2127315,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"e69d7a10-70c6-4627-cf44-ef6a08e8c9c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 404: Not Found\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 15.9MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 404: Not Found\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28.9k/28.9k [00:00<00:00, 484kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 404: Not Found\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1.65M/1.65M [00:00<00:00, 4.43MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 404: Not Found\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4.54k/4.54k [00:00<00:00, 4.84MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Epoch [1/25], Step [1/938], D Loss: 1.3777307271957397, G Loss: 0.6954929828643799\n","Epoch [1/25], Step [101/938], D Loss: 1.402376413345337, G Loss: 1.001030683517456\n","Epoch [1/25], Step [201/938], D Loss: 0.3394392430782318, G Loss: 2.0288186073303223\n","Epoch [1/25], Step [301/938], D Loss: 0.6535343527793884, G Loss: 2.72291898727417\n","Epoch [1/25], Step [401/938], D Loss: 0.31361860036849976, G Loss: 3.8078246116638184\n","Epoch [1/25], Step [501/938], D Loss: 0.2654958963394165, G Loss: 3.2202110290527344\n","Epoch [1/25], Step [601/938], D Loss: 0.28146815299987793, G Loss: 2.8612513542175293\n","Epoch [1/25], Step [701/938], D Loss: 0.3705834150314331, G Loss: 7.57533073425293\n","Epoch [1/25], Step [801/938], D Loss: 0.12982609868049622, G Loss: 2.6119747161865234\n","Epoch [1/25], Step [901/938], D Loss: 1.2019513845443726, G Loss: 8.387231826782227\n","Epoch [2/25], Step [1/938], D Loss: 0.8704684376716614, G Loss: 3.998171329498291\n","Epoch [2/25], Step [101/938], D Loss: 0.42704299092292786, G Loss: 3.1073670387268066\n","Epoch [2/25], Step [201/938], D Loss: 0.2700279951095581, G Loss: 5.204295635223389\n","Epoch [2/25], Step [301/938], D Loss: 0.4897739887237549, G Loss: 3.0879340171813965\n","Epoch [2/25], Step [401/938], D Loss: 0.10154098272323608, G Loss: 4.168497085571289\n","Epoch [2/25], Step [501/938], D Loss: 0.213567852973938, G Loss: 6.364742279052734\n","Epoch [2/25], Step [601/938], D Loss: 0.2374599575996399, G Loss: 3.4894487857818604\n","Epoch [2/25], Step [701/938], D Loss: 0.2604382038116455, G Loss: 5.166041851043701\n","Epoch [2/25], Step [801/938], D Loss: 0.3223486542701721, G Loss: 2.4806604385375977\n","Epoch [2/25], Step [901/938], D Loss: 0.20284852385520935, G Loss: 4.351906776428223\n","Epoch [3/25], Step [1/938], D Loss: 1.5198527574539185, G Loss: 12.164497375488281\n","Epoch [3/25], Step [101/938], D Loss: 0.05272076651453972, G Loss: 3.4487061500549316\n","Epoch [3/25], Step [201/938], D Loss: 0.08326558023691177, G Loss: 3.156895637512207\n","Epoch [3/25], Step [301/938], D Loss: 0.2996765971183777, G Loss: 2.511979579925537\n","Epoch [3/25], Step [401/938], D Loss: 0.24788355827331543, G Loss: 5.165582656860352\n","Epoch [3/25], Step [501/938], D Loss: 2.0455336570739746, G Loss: 16.715368270874023\n","Epoch [3/25], Step [601/938], D Loss: 0.20814314484596252, G Loss: 3.3665804862976074\n","Epoch [3/25], Step [701/938], D Loss: 0.28964465856552124, G Loss: 4.686359882354736\n","Epoch [3/25], Step [801/938], D Loss: 0.8207955956459045, G Loss: 2.14288067817688\n","Epoch [3/25], Step [901/938], D Loss: 0.265397846698761, G Loss: 4.016352653503418\n","Epoch [4/25], Step [1/938], D Loss: 1.817156195640564, G Loss: 11.64942741394043\n","Epoch [4/25], Step [101/938], D Loss: 0.2464466691017151, G Loss: 2.6314914226531982\n","Epoch [4/25], Step [201/938], D Loss: 0.1916632056236267, G Loss: 4.276975154876709\n","Epoch [4/25], Step [301/938], D Loss: 0.20244228839874268, G Loss: 4.018004894256592\n","Epoch [4/25], Step [401/938], D Loss: 0.25826504826545715, G Loss: 2.885857105255127\n","Epoch [4/25], Step [501/938], D Loss: 0.33618009090423584, G Loss: 2.798898458480835\n","Epoch [4/25], Step [601/938], D Loss: 0.4518406391143799, G Loss: 2.120770215988159\n","Epoch [4/25], Step [701/938], D Loss: 0.30295097827911377, G Loss: 2.545574426651001\n","Epoch [4/25], Step [801/938], D Loss: 0.14749784767627716, G Loss: 3.250662326812744\n","Epoch [4/25], Step [901/938], D Loss: 0.3281879425048828, G Loss: 4.077936172485352\n","Epoch [5/25], Step [1/938], D Loss: 0.22406280040740967, G Loss: 3.4623513221740723\n","Epoch [5/25], Step [101/938], D Loss: 0.8885726928710938, G Loss: 2.3161001205444336\n","Epoch [5/25], Step [201/938], D Loss: 0.2719024419784546, G Loss: 2.8220062255859375\n","Epoch [5/25], Step [301/938], D Loss: 0.4822973608970642, G Loss: 2.647460460662842\n","Epoch [5/25], Step [401/938], D Loss: 0.2988157272338867, G Loss: 2.886859178543091\n","Epoch [5/25], Step [501/938], D Loss: 0.1349012553691864, G Loss: 4.238221645355225\n","Epoch [5/25], Step [601/938], D Loss: 0.31461626291275024, G Loss: 2.453873634338379\n","Epoch [5/25], Step [701/938], D Loss: 0.20269565284252167, G Loss: 2.9814798831939697\n","Epoch [5/25], Step [801/938], D Loss: 0.46363431215286255, G Loss: 3.6018850803375244\n","Epoch [5/25], Step [901/938], D Loss: 0.25020042061805725, G Loss: 3.1303017139434814\n","Epoch [6/25], Step [1/938], D Loss: 0.3481961190700531, G Loss: 2.5632483959198\n","Epoch [6/25], Step [101/938], D Loss: 0.13672497868537903, G Loss: 3.5494384765625\n","Epoch [6/25], Step [201/938], D Loss: 0.42261451482772827, G Loss: 1.9063611030578613\n","Epoch [6/25], Step [301/938], D Loss: 0.412051260471344, G Loss: 2.33437442779541\n","Epoch [6/25], Step [401/938], D Loss: 0.37080565094947815, G Loss: 2.378995656967163\n","Epoch [6/25], Step [501/938], D Loss: 0.34702423214912415, G Loss: 3.0938007831573486\n","Epoch [6/25], Step [601/938], D Loss: 0.14899197220802307, G Loss: 3.289370059967041\n","Epoch [6/25], Step [701/938], D Loss: 0.32884764671325684, G Loss: 3.0484681129455566\n","Epoch [6/25], Step [801/938], D Loss: 0.4543831944465637, G Loss: 2.168570041656494\n","Epoch [6/25], Step [901/938], D Loss: 0.40671372413635254, G Loss: 2.504549980163574\n","Epoch [7/25], Step [1/938], D Loss: 0.3605774939060211, G Loss: 2.616421699523926\n","Epoch [7/25], Step [101/938], D Loss: 0.3499446511268616, G Loss: 3.600584030151367\n","Epoch [7/25], Step [201/938], D Loss: 0.5125246047973633, G Loss: 2.5670852661132812\n","Epoch [7/25], Step [301/938], D Loss: 0.36896032094955444, G Loss: 2.2760932445526123\n","Epoch [7/25], Step [401/938], D Loss: 0.4248751997947693, G Loss: 2.2513911724090576\n","Epoch [7/25], Step [501/938], D Loss: 0.7848978042602539, G Loss: 2.1320419311523438\n","Epoch [7/25], Step [601/938], D Loss: 0.5218505859375, G Loss: 1.4610037803649902\n","Epoch [7/25], Step [701/938], D Loss: 0.6409725546836853, G Loss: 1.8732441663742065\n","Epoch [7/25], Step [801/938], D Loss: 0.613690972328186, G Loss: 1.7276333570480347\n","Epoch [7/25], Step [901/938], D Loss: 0.7379199266433716, G Loss: 1.7695295810699463\n","Epoch [8/25], Step [1/938], D Loss: 0.6205916404724121, G Loss: 2.021710157394409\n","Epoch [8/25], Step [101/938], D Loss: 0.4565308094024658, G Loss: 2.0155231952667236\n","Epoch [8/25], Step [201/938], D Loss: 0.6639059782028198, G Loss: 2.1712639331817627\n","Epoch [8/25], Step [301/938], D Loss: 0.9594702124595642, G Loss: 4.155462265014648\n","Epoch [8/25], Step [401/938], D Loss: 0.5286594033241272, G Loss: 2.6603987216949463\n","Epoch [8/25], Step [501/938], D Loss: 0.5418256521224976, G Loss: 1.9190305471420288\n","Epoch [8/25], Step [601/938], D Loss: 0.5502588748931885, G Loss: 2.183072090148926\n","Epoch [8/25], Step [701/938], D Loss: 0.8896332383155823, G Loss: 2.39129900932312\n","Epoch [8/25], Step [801/938], D Loss: 0.5708784461021423, G Loss: 1.7370033264160156\n","Epoch [8/25], Step [901/938], D Loss: 0.73823082447052, G Loss: 2.840914249420166\n","Epoch [9/25], Step [1/938], D Loss: 0.8952653408050537, G Loss: 1.401960849761963\n","Epoch [9/25], Step [101/938], D Loss: 0.8549753427505493, G Loss: 1.622146487236023\n","Epoch [9/25], Step [201/938], D Loss: 0.7120258808135986, G Loss: 1.7064871788024902\n","Epoch [9/25], Step [301/938], D Loss: 0.6606564521789551, G Loss: 1.641181230545044\n","Epoch [9/25], Step [401/938], D Loss: 0.718193769454956, G Loss: 1.8680145740509033\n","Epoch [9/25], Step [501/938], D Loss: 0.6508859395980835, G Loss: 1.830745816230774\n","Epoch [9/25], Step [601/938], D Loss: 0.6403214931488037, G Loss: 2.320263385772705\n","Epoch [9/25], Step [701/938], D Loss: 0.6642189025878906, G Loss: 1.8062024116516113\n","Epoch [9/25], Step [801/938], D Loss: 0.8072958588600159, G Loss: 1.4291707277297974\n","Epoch [9/25], Step [901/938], D Loss: 0.7184876203536987, G Loss: 1.39280366897583\n","Epoch [10/25], Step [1/938], D Loss: 1.2458531856536865, G Loss: 3.1587259769439697\n","Epoch [10/25], Step [101/938], D Loss: 0.7379488945007324, G Loss: 2.312265634536743\n","Epoch [10/25], Step [201/938], D Loss: 0.7869843244552612, G Loss: 1.3709299564361572\n","Epoch [10/25], Step [301/938], D Loss: 0.8381049633026123, G Loss: 1.4937422275543213\n","Epoch [10/25], Step [401/938], D Loss: 0.8498225212097168, G Loss: 1.8628098964691162\n","Epoch [10/25], Step [501/938], D Loss: 0.9508526921272278, G Loss: 2.27984881401062\n","Epoch [10/25], Step [601/938], D Loss: 0.7379932999610901, G Loss: 2.0889992713928223\n","Epoch [10/25], Step [701/938], D Loss: 0.8260701894760132, G Loss: 1.7228772640228271\n","Epoch [10/25], Step [801/938], D Loss: 0.767339825630188, G Loss: 1.073272705078125\n","Epoch [10/25], Step [901/938], D Loss: 0.8948103785514832, G Loss: 2.08626651763916\n","Epoch [11/25], Step [1/938], D Loss: 0.7002279758453369, G Loss: 2.4671101570129395\n","Epoch [11/25], Step [101/938], D Loss: 0.8720764517784119, G Loss: 1.9819414615631104\n","Epoch [11/25], Step [201/938], D Loss: 0.8256230354309082, G Loss: 2.1415719985961914\n","Epoch [11/25], Step [301/938], D Loss: 0.7091791033744812, G Loss: 2.350137948989868\n","Epoch [11/25], Step [401/938], D Loss: 0.7064856290817261, G Loss: 2.29079008102417\n","Epoch [11/25], Step [501/938], D Loss: 0.9765899181365967, G Loss: 1.5941083431243896\n","Epoch [11/25], Step [601/938], D Loss: 0.9921965003013611, G Loss: 1.9932386875152588\n","Epoch [11/25], Step [701/938], D Loss: 0.8001416921615601, G Loss: 2.343529462814331\n","Epoch [11/25], Step [801/938], D Loss: 0.8578159809112549, G Loss: 1.5449223518371582\n","Epoch [11/25], Step [901/938], D Loss: 0.8539857864379883, G Loss: 1.7620627880096436\n","Epoch [12/25], Step [1/938], D Loss: 0.8104225397109985, G Loss: 1.7165218591690063\n","Epoch [12/25], Step [101/938], D Loss: 0.7551615834236145, G Loss: 1.8701945543289185\n","Epoch [12/25], Step [201/938], D Loss: 0.8217278122901917, G Loss: 2.4888908863067627\n","Epoch [12/25], Step [301/938], D Loss: 0.9369627833366394, G Loss: 1.7694833278656006\n","Epoch [12/25], Step [401/938], D Loss: 0.6731477379798889, G Loss: 1.922377347946167\n","Epoch [12/25], Step [501/938], D Loss: 0.8757548332214355, G Loss: 2.1981418132781982\n","Epoch [12/25], Step [601/938], D Loss: 0.7768819332122803, G Loss: 1.471355676651001\n","Epoch [12/25], Step [701/938], D Loss: 0.9517554044723511, G Loss: 1.7491648197174072\n","Epoch [12/25], Step [801/938], D Loss: 0.8496280908584595, G Loss: 1.2470171451568604\n","Epoch [12/25], Step [901/938], D Loss: 0.6851069927215576, G Loss: 1.9382869005203247\n","Epoch [13/25], Step [1/938], D Loss: 0.637076735496521, G Loss: 1.9450582265853882\n","Epoch [13/25], Step [101/938], D Loss: 0.8397020101547241, G Loss: 1.4346102476119995\n","Epoch [13/25], Step [201/938], D Loss: 0.900333046913147, G Loss: 2.131192922592163\n","Epoch [13/25], Step [301/938], D Loss: 0.9053912162780762, G Loss: 1.8343645334243774\n","Epoch [13/25], Step [401/938], D Loss: 0.683448314666748, G Loss: 2.0625314712524414\n","Epoch [13/25], Step [501/938], D Loss: 0.8931403160095215, G Loss: 1.738610863685608\n","Epoch [13/25], Step [601/938], D Loss: 1.0055111646652222, G Loss: 2.2779533863067627\n","Epoch [13/25], Step [701/938], D Loss: 1.0829989910125732, G Loss: 1.3679225444793701\n","Epoch [13/25], Step [801/938], D Loss: 0.6128405332565308, G Loss: 2.0388269424438477\n","Epoch [13/25], Step [901/938], D Loss: 1.163489580154419, G Loss: 0.6220900416374207\n","Epoch [14/25], Step [1/938], D Loss: 0.7907062768936157, G Loss: 1.567347764968872\n","Epoch [14/25], Step [101/938], D Loss: 0.9580577611923218, G Loss: 1.0630180835723877\n","Epoch [14/25], Step [201/938], D Loss: 0.623755693435669, G Loss: 2.014702081680298\n","Epoch [14/25], Step [301/938], D Loss: 0.8248782157897949, G Loss: 2.3990554809570312\n","Epoch [14/25], Step [401/938], D Loss: 0.7942204475402832, G Loss: 2.0551047325134277\n","Epoch [14/25], Step [501/938], D Loss: 0.721131443977356, G Loss: 1.773756504058838\n","Epoch [14/25], Step [601/938], D Loss: 0.9449106454849243, G Loss: 1.798301100730896\n","Epoch [14/25], Step [701/938], D Loss: 0.9638700485229492, G Loss: 1.047371745109558\n","Epoch [14/25], Step [801/938], D Loss: 1.0486171245574951, G Loss: 0.7601637244224548\n","Epoch [14/25], Step [901/938], D Loss: 0.7770476341247559, G Loss: 1.8512980937957764\n","Epoch [15/25], Step [1/938], D Loss: 0.8768384456634521, G Loss: 1.5624732971191406\n","Epoch [15/25], Step [101/938], D Loss: 0.7655693292617798, G Loss: 1.7937431335449219\n","Epoch [15/25], Step [201/938], D Loss: 0.7275322675704956, G Loss: 1.9256985187530518\n","Epoch [15/25], Step [301/938], D Loss: 1.1055948734283447, G Loss: 0.8474067449569702\n","Epoch [15/25], Step [401/938], D Loss: 0.8455194234848022, G Loss: 1.6192386150360107\n","Epoch [15/25], Step [501/938], D Loss: 0.9261941909790039, G Loss: 1.2880948781967163\n","Epoch [15/25], Step [601/938], D Loss: 0.9358038306236267, G Loss: 2.628084659576416\n","Epoch [15/25], Step [701/938], D Loss: 0.6207208633422852, G Loss: 1.8234906196594238\n","Epoch [15/25], Step [801/938], D Loss: 0.9768980145454407, G Loss: 1.1231402158737183\n","Epoch [15/25], Step [901/938], D Loss: 0.8770129680633545, G Loss: 1.4266884326934814\n","Epoch [16/25], Step [1/938], D Loss: 0.8439533710479736, G Loss: 1.8091706037521362\n","Epoch [16/25], Step [101/938], D Loss: 0.8842015862464905, G Loss: 1.6634724140167236\n","Epoch [16/25], Step [201/938], D Loss: 0.855048418045044, G Loss: 1.5328885316848755\n","Epoch [16/25], Step [301/938], D Loss: 0.9224306344985962, G Loss: 2.0055220127105713\n","Epoch [16/25], Step [401/938], D Loss: 0.9807273149490356, G Loss: 1.5141894817352295\n","Epoch [16/25], Step [501/938], D Loss: 0.9293793439865112, G Loss: 2.1177456378936768\n","Epoch [16/25], Step [601/938], D Loss: 0.8407005667686462, G Loss: 1.210721492767334\n","Epoch [16/25], Step [701/938], D Loss: 0.8724769353866577, G Loss: 2.394503116607666\n","Epoch [16/25], Step [801/938], D Loss: 0.8871612548828125, G Loss: 1.2165635824203491\n","Epoch [16/25], Step [901/938], D Loss: 0.9220913052558899, G Loss: 2.0202178955078125\n","Epoch [17/25], Step [1/938], D Loss: 0.8817507028579712, G Loss: 1.8002301454544067\n","Epoch [17/25], Step [101/938], D Loss: 0.8979596495628357, G Loss: 1.9667975902557373\n","Epoch [17/25], Step [201/938], D Loss: 1.0905978679656982, G Loss: 1.6569069623947144\n","Epoch [17/25], Step [301/938], D Loss: 0.9276410341262817, G Loss: 1.4756568670272827\n","Epoch [17/25], Step [401/938], D Loss: 0.7725609540939331, G Loss: 1.6021393537521362\n","Epoch [17/25], Step [501/938], D Loss: 0.7770828604698181, G Loss: 1.6331114768981934\n","Epoch [17/25], Step [601/938], D Loss: 0.8205733299255371, G Loss: 1.8673211336135864\n","Epoch [17/25], Step [701/938], D Loss: 0.900375485420227, G Loss: 1.6317956447601318\n","Epoch [17/25], Step [801/938], D Loss: 0.838386058807373, G Loss: 1.3587392568588257\n","Epoch [17/25], Step [901/938], D Loss: 0.988001823425293, G Loss: 1.5646761655807495\n","Epoch [18/25], Step [1/938], D Loss: 0.8571882843971252, G Loss: 1.9907281398773193\n","Epoch [18/25], Step [101/938], D Loss: 0.7745446562767029, G Loss: 1.6056666374206543\n","Epoch [18/25], Step [201/938], D Loss: 1.097676157951355, G Loss: 0.7242051959037781\n","Epoch [18/25], Step [301/938], D Loss: 0.9547956585884094, G Loss: 1.3268628120422363\n","Epoch [18/25], Step [401/938], D Loss: 0.7621333599090576, G Loss: 1.2063164710998535\n","Epoch [18/25], Step [501/938], D Loss: 1.1345102787017822, G Loss: 2.137984275817871\n","Epoch [18/25], Step [601/938], D Loss: 0.8141916990280151, G Loss: 1.6688827276229858\n","Epoch [18/25], Step [701/938], D Loss: 1.0443668365478516, G Loss: 0.935822606086731\n","Epoch [18/25], Step [801/938], D Loss: 0.8280786871910095, G Loss: 2.0805394649505615\n","Epoch [18/25], Step [901/938], D Loss: 0.963077187538147, G Loss: 1.3741728067398071\n","Epoch [19/25], Step [1/938], D Loss: 0.9721511006355286, G Loss: 1.3041932582855225\n","Epoch [19/25], Step [101/938], D Loss: 0.7558174133300781, G Loss: 1.7017974853515625\n","Epoch [19/25], Step [201/938], D Loss: 0.8074678182601929, G Loss: 2.0072762966156006\n","Epoch [19/25], Step [301/938], D Loss: 0.9682168960571289, G Loss: 1.2555210590362549\n","Epoch [19/25], Step [401/938], D Loss: 0.9013247489929199, G Loss: 1.3715548515319824\n","Epoch [19/25], Step [501/938], D Loss: 0.8398492932319641, G Loss: 1.5547325611114502\n","Epoch [19/25], Step [601/938], D Loss: 0.9095482230186462, G Loss: 1.1749142408370972\n","Epoch [19/25], Step [701/938], D Loss: 0.8454229831695557, G Loss: 1.825751543045044\n","Epoch [19/25], Step [801/938], D Loss: 0.8294404745101929, G Loss: 1.2034783363342285\n","Epoch [19/25], Step [901/938], D Loss: 0.7611936926841736, G Loss: 2.037903308868408\n","Epoch [20/25], Step [1/938], D Loss: 0.9095986485481262, G Loss: 2.60642671585083\n","Epoch [20/25], Step [101/938], D Loss: 0.8241199254989624, G Loss: 1.9971013069152832\n","Epoch [20/25], Step [201/938], D Loss: 0.8676401376724243, G Loss: 1.5652525424957275\n","Epoch [20/25], Step [301/938], D Loss: 0.8761097192764282, G Loss: 1.8411229848861694\n","Epoch [20/25], Step [401/938], D Loss: 0.7908380627632141, G Loss: 1.6720516681671143\n","Epoch [20/25], Step [501/938], D Loss: 0.8351545333862305, G Loss: 1.7192447185516357\n","Epoch [20/25], Step [601/938], D Loss: 0.8305416107177734, G Loss: 1.5597543716430664\n","Epoch [20/25], Step [701/938], D Loss: 0.9510861039161682, G Loss: 1.2607566118240356\n","Epoch [20/25], Step [801/938], D Loss: 0.9512525200843811, G Loss: 1.4802441596984863\n","Epoch [20/25], Step [901/938], D Loss: 0.866822361946106, G Loss: 2.2691650390625\n","Epoch [21/25], Step [1/938], D Loss: 0.8433393239974976, G Loss: 1.7955107688903809\n","Epoch [21/25], Step [101/938], D Loss: 0.7810821533203125, G Loss: 1.893904209136963\n","Epoch [21/25], Step [201/938], D Loss: 0.8601602911949158, G Loss: 1.6220104694366455\n","Epoch [21/25], Step [301/938], D Loss: 1.0791454315185547, G Loss: 1.9721120595932007\n","Epoch [21/25], Step [401/938], D Loss: 1.0088576078414917, G Loss: 1.7231320142745972\n","Epoch [21/25], Step [501/938], D Loss: 0.8514779210090637, G Loss: 1.2266409397125244\n","Epoch [21/25], Step [601/938], D Loss: 0.8603928089141846, G Loss: 1.5608254671096802\n","Epoch [21/25], Step [701/938], D Loss: 0.7708681225776672, G Loss: 1.6309218406677246\n","Epoch [21/25], Step [801/938], D Loss: 0.9009295105934143, G Loss: 1.7104523181915283\n","Epoch [21/25], Step [901/938], D Loss: 0.949944019317627, G Loss: 1.7302806377410889\n","Epoch [22/25], Step [1/938], D Loss: 0.8373743891716003, G Loss: 1.4818439483642578\n","Epoch [22/25], Step [101/938], D Loss: 0.9817144870758057, G Loss: 1.5488853454589844\n","Epoch [22/25], Step [201/938], D Loss: 0.8553110957145691, G Loss: 1.4857802391052246\n","Epoch [22/25], Step [301/938], D Loss: 0.8124685287475586, G Loss: 1.7175500392913818\n","Epoch [22/25], Step [401/938], D Loss: 0.840319812297821, G Loss: 2.2650692462921143\n","Epoch [22/25], Step [501/938], D Loss: 0.860740065574646, G Loss: 1.77806556224823\n","Epoch [22/25], Step [601/938], D Loss: 1.0831011533737183, G Loss: 2.2476155757904053\n","Epoch [22/25], Step [701/938], D Loss: 0.8474565744400024, G Loss: 1.8486826419830322\n","Epoch [22/25], Step [801/938], D Loss: 0.7672287225723267, G Loss: 1.7911241054534912\n","Epoch [22/25], Step [901/938], D Loss: 0.9483312964439392, G Loss: 1.787675142288208\n","Epoch [23/25], Step [1/938], D Loss: 0.8582959771156311, G Loss: 1.5194671154022217\n","Epoch [23/25], Step [101/938], D Loss: 0.78724604845047, G Loss: 2.09287166595459\n","Epoch [23/25], Step [201/938], D Loss: 0.7072657346725464, G Loss: 2.112277030944824\n","Epoch [23/25], Step [301/938], D Loss: 0.8022047877311707, G Loss: 1.6216388940811157\n","Epoch [23/25], Step [401/938], D Loss: 0.9732968807220459, G Loss: 1.515614628791809\n","Epoch [23/25], Step [501/938], D Loss: 0.9496653079986572, G Loss: 2.17832612991333\n","Epoch [23/25], Step [601/938], D Loss: 0.9041842818260193, G Loss: 1.2892380952835083\n","Epoch [23/25], Step [701/938], D Loss: 0.884856104850769, G Loss: 2.347520351409912\n","Epoch [23/25], Step [801/938], D Loss: 0.7362160682678223, G Loss: 1.770370364189148\n","Epoch [23/25], Step [901/938], D Loss: 0.6745731830596924, G Loss: 1.8808610439300537\n","Epoch [24/25], Step [1/938], D Loss: 0.9525690674781799, G Loss: 1.868953824043274\n","Epoch [24/25], Step [101/938], D Loss: 1.1067993640899658, G Loss: 2.4666855335235596\n","Epoch [24/25], Step [201/938], D Loss: 0.958310604095459, G Loss: 2.018648386001587\n","Epoch [24/25], Step [301/938], D Loss: 0.8982449769973755, G Loss: 1.580376386642456\n","Epoch [24/25], Step [401/938], D Loss: 0.9267885684967041, G Loss: 1.7879505157470703\n","Epoch [24/25], Step [501/938], D Loss: 0.7969731688499451, G Loss: 1.843112587928772\n","Epoch [24/25], Step [601/938], D Loss: 0.9668142795562744, G Loss: 1.7909290790557861\n","Epoch [24/25], Step [701/938], D Loss: 0.9087024927139282, G Loss: 1.674233078956604\n","Epoch [24/25], Step [801/938], D Loss: 0.8690213561058044, G Loss: 1.7463843822479248\n","Epoch [24/25], Step [901/938], D Loss: 0.8215998411178589, G Loss: 1.6162798404693604\n","Epoch [25/25], Step [1/938], D Loss: 1.1675299406051636, G Loss: 0.9705809354782104\n","Epoch [25/25], Step [101/938], D Loss: 0.8433133363723755, G Loss: 1.6494601964950562\n","Epoch [25/25], Step [201/938], D Loss: 0.8693623542785645, G Loss: 1.8600395917892456\n","Epoch [25/25], Step [301/938], D Loss: 0.7755306959152222, G Loss: 1.5288581848144531\n","Epoch [25/25], Step [401/938], D Loss: 0.7719612121582031, G Loss: 1.5590232610702515\n","Epoch [25/25], Step [501/938], D Loss: 0.9967663288116455, G Loss: 1.425966501235962\n","Epoch [25/25], Step [601/938], D Loss: 0.850079357624054, G Loss: 1.7366830110549927\n","Epoch [25/25], Step [701/938], D Loss: 0.8662711977958679, G Loss: 1.5247143507003784\n","Epoch [25/25], Step [801/938], D Loss: 0.875622034072876, G Loss: 1.4255601167678833\n","Epoch [25/25], Step [901/938], D Loss: 0.8227684497833252, G Loss: 1.4807448387145996\n","Training completed!\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from datasets import load_dataset\n","import torchvision.transforms as transforms\n","from torchvision import datasets\n","from torchvision.utils import save_image\n","\n","# Define the Generator Model\n","class Generator(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(Generator, self).__init__()\n","        self.fc1 = nn.Linear(latent_dim, 256)\n","        self.fc2 = nn.Linear(256, 512)\n","        self.fc3 = nn.Linear(512, 1024)\n","        self.fc4 = nn.Linear(1024, 28 * 28)  # MNIST image size\n","        self.relu = nn.ReLU()\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, z):\n","        x = self.relu(self.fc1(z))\n","        x = self.relu(self.fc2(x))\n","        x = self.relu(self.fc3(x))\n","        x = self.tanh(self.fc4(x))  # Output between [-1, 1]\n","        return x.view(x.size(0), 1, 28, 28)  # Reshape to image\n","\n","# Define the Discriminator Model\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.fc1 = nn.Linear(28 * 28, 1024)\n","        self.fc2 = nn.Linear(1024, 512)\n","        self.fc3 = nn.Linear(512, 256)\n","        self.fc4 = nn.Linear(256, 1)  # Single output: real or fake\n","        self.leaky_relu = nn.LeakyReLU(0.2)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)  # Flatten image\n","        x = self.leaky_relu(self.fc1(x))\n","        x = self.leaky_relu(self.fc2(x))\n","        x = self.leaky_relu(self.fc3(x))\n","        x = self.sigmoid(self.fc4(x))\n","        return x\n","\n","# Initialize models\n","latent_dim = 100  # Latent space dimension for generator\n","generator = Generator(latent_dim)\n","discriminator = Discriminator()\n","\n","# Optimizers\n","lr = 0.0002\n","betas = (0.5, 0.999)\n","optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=betas)\n","optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n","\n","# Loss function\n","criterion = nn.BCELoss()\n","\n","# Load Dataset (MNIST)\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n","])\n","\n","dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n","\n","# Training Loop\n","num_epochs = 25\n","\n","for epoch in range(num_epochs):\n","    for i, (imgs, _) in enumerate(dataloader):\n","        batch_size = imgs.size(0)\n","        real_imgs = imgs\n","\n","        # Create labels for real and fake images\n","        real_labels = torch.ones(batch_size, 1)\n","        fake_labels = torch.zeros(batch_size, 1)\n","\n","        # Train Discriminator\n","        optimizer_D.zero_grad()\n","\n","        # Real images\n","        real_outputs = discriminator(real_imgs)\n","        d_loss_real = criterion(real_outputs, real_labels)\n","\n","        # Fake images generated by the generator\n","        z = torch.randn(batch_size, latent_dim)\n","        fake_imgs = generator(z)\n","        fake_outputs = discriminator(fake_imgs.detach())  # Detach to avoid backprop through generator\n","        d_loss_fake = criterion(fake_outputs, fake_labels)\n","\n","        # Total discriminator loss\n","        d_loss = d_loss_real + d_loss_fake\n","        d_loss.backward()\n","        optimizer_D.step()\n","\n","        # Train Generator\n","        optimizer_G.zero_grad()\n","\n","        # We want the generator to fool the discriminator (fake images should be classified as real)\n","        outputs = discriminator(fake_imgs)\n","        g_loss = criterion(outputs, real_labels)\n","        g_loss.backward()\n","        optimizer_G.step()\n","\n","        if i % 100 == 0:\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], \"\n","                  f\"D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n","\n","    # Save generated images every epoch\n","    save_image(fake_imgs.data[:25], f\"generated_images_epoch_{epoch+1}.png\", nrow=5, normalize=True)\n","\n","print(\"Training completed!\")\n"]},{"cell_type":"code","source":["import torch\n","import torchvision.utils as vutils\n","import matplotlib.pyplot as plt\n","\n","# Assuming the generator is already trained\n","latent_dim = 100  # Same latent space dimension as used during training\n","num_images = 3 # Number of images to generate\n","\n","# Generate random latent vectors (noise) for fake image generation\n","z = torch.randn(num_images, latent_dim)\n","\n","# Generate images from the random noise\n","generated_images = generator(z)\n","\n","# Create a grid of images\n","grid = vutils.make_grid(generated_images, nrow=5, normalize=True)\n","\n","# Convert the grid to a numpy array and plot using matplotlib\n","plt.figure(figsize=(5, 5))\n","plt.imshow(grid.permute(1, 2, 0).detach().numpy())  # Remove the batch dimension and plot\n","plt.axis('off')  # Turn off axis\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"id":"R74Dlg2RLSpZ","executionInfo":{"status":"ok","timestamp":1741847187297,"user_tz":-330,"elapsed":103,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"01fd5fb7-023a-40b2-e78b-b0c1576aa0f1"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 500x500 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZcAAACaCAYAAABljwVnAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAC3hJREFUeJzt3dlvTP8fx/FjqVLVaquoLYi1uJBYg7gQEUm5IBJuRFRIhVgiIhHiL7BfCBIiREQvUCIhKg1BQ6hoEaktloaKpVW6KL/b7/t9zq9n5sx75kzb5+PudTo98+mMmbc57/l8Pl3+/fv3zwEAwFDXsAcAAOh4KC4AAHMUFwCAOYoLAMAcxQUAYI7iAgAwR3EBAJijuAAAzHWP9IZdunSJ5zgAAO1EJHPv+eQCADBHcQEAmKO4AADMUVwAAOYoLgAAcxQXAIA5igsAwFzE81yAREpJSRG5paUlpJEACIJPLgAAcxQXAIA5igsAwBzFBQBgrsu/SFYgc1i4EkBy2rhxo8iHDh0KaSSdBwtXAgBCQXEBAJijuAAAzNFzAQBEhZ4LACAUFBcAgDmKCwDAHAtXAgjEqw8bYQsXnQCfXAAA5iguAABzFBcAgDl6LkDIdO/Ca2O0Hz9+iJyTkxPXMUWiPfVXJk+eLPKRI0dEfvbsmcirVq2K+j5SU1NFbmpqivocHQmfXAAA5iguAABzFBcAgDmKCwDAHAtXAiH7+/ev720aGxtFLikpEXnt2rUi6y8AdGT68auqqnLd5vjx4yLn5eWJvH379qjvV7913rlzR+Q5c+ZEfc72goUrAQChoLgAAMxRXAAA5phECSRYkH5IZWWlyBcuXIj5nImQnp4u8s+fP83vY+LEiSIXFha6brNv3z7z+9V96FmzZonc3Nwssu6TOY7jLF261HxcyYJPLgAAcxQXAIA5igsAwBzzXDz07dtX5O/fv4cyDu3SpUsiFxQUiJyRkeH6nXhc40Z0evXqJbJ+TiJ5bT158kTkadOmiawXu4xk7kx71bWr/D+x7rlUVFS4fkc/xmEsutnQ0OA6NnDgQN/bJCPmuQAAQkFxAQCYo7gAAMwxz8VxnMzMTJH12kTXr18XuaioyHWO379/m4+rtbVVZL9r83V1da5j+vo04svr8S4uLhY5SP9S/1voTBtR6ce0Z8+eInv1WDS/HsGfP39E7t7d/q0xLS3NdWzAgAEiv379WuT2tCGbxjsPAMAcxQUAYI7iAgAw1+F7LgcOHHAd27hxY5u/o+chXLt2TWS9ZlC86Gvzft/Vp78Svjdv3riODRkyJKpz6PlMjtOx16Dyo+fsxGPuVm1trch9+vRx3ebXr19tjmP48OEi69ejV6/txYsXIus5df379xc5HvOX9Dwsx7HpIfNuBAAwR3EBAJijuAAAzFFcAADm2t3ClfX19SL37t3b/D70xktZWVnm9xGJaJt3NPTjb+jQoSI/fPhQ5JycnJjvg+dRSklJEVlPIA2yKOWjR49Enj59ush6UmUQ+gsAXk1yvdhst27d2jxneXm5yDNnzgw4utiwcCUAIBQUFwCAOYoLAMBcUvVcvCag6euteXl5IuvhBxmnngzltelWGPx6LlybT7yamhqR9cKDkdAT5YYNGyZykEmC+nWiNw9rT3TfQfcqgiwquX79epGPHTsmsl4YNFH05mD6efT7W8N6D6DnAgAIBcUFAGCO4gIAMJdUC1fqhd8cx3FKS0tF1j2XyspKkXfu3CnysmXLXOdcvXq1yPv3749ilPET5Pv6/+X1HfmwriV3FHpjKq8FDf3o53H8+PEiWyzE2J57LFq/fv1E9pv7oa1bt851TPdYkoWep6dfr36bmHn1XPx6tTNmzBD53r17vuMMgk8uAABzFBcAgDmKCwDAXFLNc3n+/Lnr2JgxY9r8HYvveeu/Tc9z0WuNWUhPT3cd0/2lSZMmiayvt+rvxMOefu6D9FzKyspEnjdvnsjx2ACqPdOPz+nTp0X2m1uk5xE5juNkZ2fHPK5E0O89ehMz/ZpnngsAoFOhuAAAzFFcAADmkmqey+jRo13Hbt26JXJmZqb5/errh0F6LNHOUfG6D32Oy5cvi2wxHwJtq6qqEjnaHovX815YWCgyPZa26R5VdXW1yH49l/z8fNcx3ZtI1ufg1atXIr97907kkSNHirx3717XObZu3Wo/sAD45AIAMEdxAQCYo7gAAMxRXAAA5pJqEqUXv8abXtQu2sUeg9AbDzmO4xw5ckRk/XjpZnxqaqrv/ei/PScnR+R4TO7sTNLS0lzHYv3SxJUrV1zHFi1aFNM5Oxu/17RezFH79euX61h5ebnIeqJmsrh586bIc+fOFVk/Fs3Nza5zFBQUiHzjxg2j0f3/cXjhkwsAwBzFBQBgjuICADCXVJMovejNc/RkqPr6epF1n0IvBBeJHTt2iLxy5UqRvSYu6WujDx48EDmSHoumN4Cix2Lr5cuXMZ9DX/9fsmRJzOdMFP3ainZTrnjx2+Du+vXrIs+fP19kr17aqVOnYh9YAsyePbvNn+vHZtSoUa7bDB48WGT9vCZqA0E+uQAAzFFcAADmKC4AAHNJP8+ld+/eIuuF3XJzc0XWf47Xn6f7Nn7XnvU5vB4LvanPnDlzRF6xYoXIu3fvdp1DS5Zr4O2Vfvw+fPggcv/+/aM+p/63MmTIEJE/ffoU9TnRNr1B1sePH0XW87+81NXVidy3b9+Yx2XhzJkzIuv3CT9evdz379+L7LfQZxDMcwEAhILiAgAwR3EBAJhL+p5LtPRaO927+0/lefz4sch63aGGhgaRvb6LXlpaKvK2bdtEzsvLE/nNmzeucxw8eNB3rIicnscyYsSIqM/R2Ngo8v3790XW85sQf1+/fhVZ90+83qv025x+/elNuLz4bQiof37ixAmRdb/YcRxnz549vvf7X3oen+5HeY3Lj9d7pN/6bfRcAAChoLgAAMxRXAAA5iguAABzSb9wZbR69OgR9/vw2nxHfwlA30YvZLlr1y77gXVyQb7M4aesrExkNv4Kn/5ixrdv30T2ajbrZvvw4cNF1o1yrwm2R48eFfn06dMiX7x4UWT9b+fw4cOuc0br7du3Ii9dutR1m+Li4qjO6de8D4pPLgAAcxQXAIA5igsAwFyHm0SZCF4LSn758kXkPn36iKwXy9QZ0VmzZo3rmL4mHoTefC4/P19kvfglwqd7BhavLa+3RX3swIEDIutNBbOzs0W2eA9NlvcNJlECAEJBcQEAmKO4AADMdbh5LomQlpbmOpaRkSGyvr5qMeeiM9OP58KFC2M+p9d1Y72xkl64EslHv7Z+/vzpuk2vXr1E9ut/eP1cHysqKhK5Z8+ebZ4zEk1NTSKXlJTEfM6w8MkFAGCO4gIAMEdxAQCYoxEQgJ7T4jju67F6rSKdEZtp06ZF/Tv6OZgyZYrrNu21xzJ48GCRO/N8nKysLNcxvbbfpEmTYr6f6upqkceOHSvy4sWLRb569arvOadOnSry06dPA44ufHxyAQCYo7gAAMxRXAAA5ui5BJCSkuI6dvbsWZG3bNmSqOF0CnpOyoIFC1y3OX/+vMjp6ekiL1myROSKigqbwSWBztxj0VpaWlzH9u/fL/KECRNE3rRpk8jv3793nSMvL0/kzZs3i3z37l2RN2zYIPL9+/dd5zx58qTIlZWVIk+fPl3k8vJy1zmSFZ9cAADmKC4AAHMUFwCAOYoLAMAcm4UFMGjQINexmpoakSN8WGFIT4ocN26cyOfOnRPZq/ELW3pjvdbW1pBGEp3Zs2e7jr169UpkvUnZ58+f4zqmZMJmYQCAUFBcAADmKC4AAHP0XCKwfPlykb028GloaEjUcIC4yM3NFbm2tjakkSDZ0XMBAISC4gIAMEdxAQCYo+cSgNd34G/fvh3CSAAg8ei5AABCQXEBAJijuAAAzNFzAQBEhZ4LACAUFBcAgDmKCwDAHMUFAGCO4gIAMEdxAQCYo7gAAMxRXAAA5iguAABzFBcAgDmKCwDAHMUFAGCue6Q3jHB9SwAA+OQCALBHcQEAmKO4AADMUVwAAOYoLgAAcxQXAIA5igsAwBzFBQBgjuICADD3P+nxbCYaCw1bAAAAAElFTkSuQmCC\n"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"sue5ilypLULq"},"execution_count":null,"outputs":[]}]}