{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNyo+ndmGOtbSukLc0O42q0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install google-generativeai\n","!pip install faiss-cpu\n","!pip install transformers torchaudio librosa\n","!pip install git+https://github.com/salesforce/LAVIS.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u_-sSz5MNE-Y","executionInfo":{"status":"ok","timestamp":1743827949019,"user_tz":-330,"elapsed":25226,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"6aedbe80-0b1c-429f-e166-3efc57f30cb9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.69.2)\n","Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n","Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.6.0+cu124)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (4.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchaudio) (1.3.0)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n","Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n","Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchaudio) (3.0.2)\n","Collecting git+https://github.com/salesforce/LAVIS.git\n","  Cloning https://github.com/salesforce/LAVIS.git to /tmp/pip-req-build-ef3xpavo\n","  Running command git clone --filter=blob:none --quiet https://github.com/salesforce/LAVIS.git /tmp/pip-req-build-ef3xpavo\n","  Resolved https://github.com/salesforce/LAVIS.git to commit 506965b9c4a18c1e565bd32acaccabe0198433f7\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting contexttimer (from salesforce-lavis==1.0.1)\n","  Using cached contexttimer-0.3.3.tar.gz (4.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting decord (from salesforce-lavis==1.0.1)\n","  Using cached decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n","Collecting diffusers<=0.16.0 (from salesforce-lavis==1.0.1)\n","  Using cached diffusers-0.16.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: einops>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.8.1)\n","Collecting fairscale==0.4.4 (from salesforce-lavis==1.0.1)\n","  Using cached fairscale-0.4.4.tar.gz (235 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy (from salesforce-lavis==1.0.1)\n","  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting iopath (from salesforce-lavis==1.0.1)\n","  Using cached iopath-0.1.10.tar.gz (42 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (7.34.0)\n","Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.3.0)\n","Collecting opencv-python-headless==4.5.5.64 (from salesforce-lavis==1.0.1)\n","  Using cached opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Collecting opendatasets (from salesforce-lavis==1.0.1)\n","  Using cached opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (24.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.2.2)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (5.24.1)\n","Collecting pre-commit (from salesforce-lavis==1.0.1)\n","  Using cached pre_commit-4.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting pycocoevalcap (from salesforce-lavis==1.0.1)\n","  Using cached pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.0.8)\n","Collecting python-magic (from salesforce-lavis==1.0.1)\n","  Using cached python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.25.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.2.0)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (3.8.5)\n","Collecting streamlit (from salesforce-lavis==1.0.1)\n","  Using cached streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n","Collecting timm==0.4.12 (from salesforce-lavis==1.0.1)\n","  Using cached timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.21.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (4.67.1)\n","Collecting transformers==4.33.2 (from salesforce-lavis==1.0.1)\n","  Using cached transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\n","Collecting webdataset (from salesforce-lavis==1.0.1)\n","  Using cached webdataset-0.2.111-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.45.1)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.6.0+cu124)\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.13.1)\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (1.0.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (3.9.1)\n","Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.14.0)\n","Collecting easydict==1.9 (from salesforce-lavis==1.0.1)\n","  Using cached easydict-1.9.tar.gz (6.4 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pyyaml_env_tag==0.1 (from salesforce-lavis==1.0.1)\n","  Using cached pyyaml_env_tag-0.1-py3-none-any.whl.metadata (4.1 kB)\n","INFO: pip is looking at multiple versions of salesforce-lavis to determine which version is compatible with other requirements. This could take a while.\n","\u001b[31mERROR: Ignored the following versions that require a different python version: 0.55.2 Requires-Python <3.5\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement open3d==0.13.0 (from salesforce-lavis) (from versions: 0.18.0, 0.19.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for open3d==0.13.0\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install git+https://github.com/salesforce/LAVIS.git\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BKMQ7MOaNU-P","executionInfo":{"status":"ok","timestamp":1743827975971,"user_tz":-330,"elapsed":26949,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"6ccee2cc-02f8-4d81-c1cd-54f52e3778ca"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/salesforce/LAVIS.git\n","  Cloning https://github.com/salesforce/LAVIS.git to /tmp/pip-req-build-in_h78i9\n","  Running command git clone --filter=blob:none --quiet https://github.com/salesforce/LAVIS.git /tmp/pip-req-build-in_h78i9\n","  Resolved https://github.com/salesforce/LAVIS.git to commit 506965b9c4a18c1e565bd32acaccabe0198433f7\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting contexttimer (from salesforce-lavis==1.0.1)\n","  Using cached contexttimer-0.3.3.tar.gz (4.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting decord (from salesforce-lavis==1.0.1)\n","  Using cached decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n","Collecting diffusers<=0.16.0 (from salesforce-lavis==1.0.1)\n","  Using cached diffusers-0.16.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: einops>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.8.1)\n","Collecting fairscale==0.4.4 (from salesforce-lavis==1.0.1)\n","  Using cached fairscale-0.4.4.tar.gz (235 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy (from salesforce-lavis==1.0.1)\n","  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting iopath (from salesforce-lavis==1.0.1)\n","  Using cached iopath-0.1.10.tar.gz (42 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (7.34.0)\n","Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.3.0)\n","Collecting opencv-python-headless==4.5.5.64 (from salesforce-lavis==1.0.1)\n","  Using cached opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Collecting opendatasets (from salesforce-lavis==1.0.1)\n","  Using cached opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (24.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.2.2)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (5.24.1)\n","Collecting pre-commit (from salesforce-lavis==1.0.1)\n","  Using cached pre_commit-4.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting pycocoevalcap (from salesforce-lavis==1.0.1)\n","  Using cached pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.0.8)\n","Collecting python-magic (from salesforce-lavis==1.0.1)\n","  Using cached python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.25.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.2.0)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (3.8.5)\n","Collecting streamlit (from salesforce-lavis==1.0.1)\n","  Using cached streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n","Collecting timm==0.4.12 (from salesforce-lavis==1.0.1)\n","  Using cached timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.21.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (4.67.1)\n","Collecting transformers==4.33.2 (from salesforce-lavis==1.0.1)\n","  Using cached transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\n","Collecting webdataset (from salesforce-lavis==1.0.1)\n","  Using cached webdataset-0.2.111-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.45.1)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.6.0+cu124)\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.13.1)\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (1.0.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (3.9.1)\n","Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.14.0)\n","Collecting easydict==1.9 (from salesforce-lavis==1.0.1)\n","  Using cached easydict-1.9.tar.gz (6.4 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pyyaml_env_tag==0.1 (from salesforce-lavis==1.0.1)\n","  Using cached pyyaml_env_tag-0.1-py3-none-any.whl.metadata (4.1 kB)\n","INFO: pip is looking at multiple versions of salesforce-lavis to determine which version is compatible with other requirements. This could take a while.\n","\u001b[31mERROR: Ignored the following versions that require a different python version: 0.55.2 Requires-Python <3.5\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement open3d==0.13.0 (from salesforce-lavis) (from versions: 0.18.0, 0.19.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for open3d==0.13.0\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["\n","!pip install google-generativeai faiss-cpu transformers torchaudio librosa\n","!pip install git+https://github.com/salesforce/LAVIS.git\n","!pip install timm fairscale omegaconf\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ApDYzHkPOzO5","executionInfo":{"status":"ok","timestamp":1743828006432,"user_tz":-330,"elapsed":30460,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"4acd9c25-b123-45c0-d583-e7062c4b9db7"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.6.0+cu124)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchaudio) (1.3.0)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n","Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n","Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.69.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n","Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchaudio) (3.0.2)\n","Collecting git+https://github.com/salesforce/LAVIS.git\n","  Cloning https://github.com/salesforce/LAVIS.git to /tmp/pip-req-build-8liw3gg8\n","  Running command git clone --filter=blob:none --quiet https://github.com/salesforce/LAVIS.git /tmp/pip-req-build-8liw3gg8\n","  Resolved https://github.com/salesforce/LAVIS.git to commit 506965b9c4a18c1e565bd32acaccabe0198433f7\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting contexttimer (from salesforce-lavis==1.0.1)\n","  Using cached contexttimer-0.3.3.tar.gz (4.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting decord (from salesforce-lavis==1.0.1)\n","  Using cached decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n","Collecting diffusers<=0.16.0 (from salesforce-lavis==1.0.1)\n","  Using cached diffusers-0.16.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: einops>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.8.1)\n","Collecting fairscale==0.4.4 (from salesforce-lavis==1.0.1)\n","  Using cached fairscale-0.4.4.tar.gz (235 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy (from salesforce-lavis==1.0.1)\n","  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting iopath (from salesforce-lavis==1.0.1)\n","  Using cached iopath-0.1.10.tar.gz (42 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (7.34.0)\n","Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.3.0)\n","Collecting opencv-python-headless==4.5.5.64 (from salesforce-lavis==1.0.1)\n","  Using cached opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Collecting opendatasets (from salesforce-lavis==1.0.1)\n","  Using cached opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (24.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.2.2)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (5.24.1)\n","Collecting pre-commit (from salesforce-lavis==1.0.1)\n","  Using cached pre_commit-4.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting pycocoevalcap (from salesforce-lavis==1.0.1)\n","  Using cached pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.0.8)\n","Collecting python-magic (from salesforce-lavis==1.0.1)\n","  Using cached python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.25.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.2.0)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (3.8.5)\n","Collecting streamlit (from salesforce-lavis==1.0.1)\n","  Using cached streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n","Collecting timm==0.4.12 (from salesforce-lavis==1.0.1)\n","  Using cached timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.21.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (4.67.1)\n","Collecting transformers==4.33.2 (from salesforce-lavis==1.0.1)\n","  Using cached transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\n","Collecting webdataset (from salesforce-lavis==1.0.1)\n","  Using cached webdataset-0.2.111-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.45.1)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.6.0+cu124)\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.13.1)\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (1.0.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (3.9.1)\n","Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.14.0)\n","Collecting easydict==1.9 (from salesforce-lavis==1.0.1)\n","  Using cached easydict-1.9.tar.gz (6.4 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pyyaml_env_tag==0.1 (from salesforce-lavis==1.0.1)\n","  Using cached pyyaml_env_tag-0.1-py3-none-any.whl.metadata (4.1 kB)\n","INFO: pip is looking at multiple versions of salesforce-lavis to determine which version is compatible with other requirements. This could take a while.\n","\u001b[31mERROR: Ignored the following versions that require a different python version: 0.55.2 Requires-Python <3.5\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement open3d==0.13.0 (from salesforce-lavis) (from versions: 0.18.0, 0.19.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for open3d==0.13.0\u001b[0m\u001b[31m\n","\u001b[0mRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n","Requirement already satisfied: fairscale in /usr/local/lib/python3.11/dist-packages (0.4.13)\n","Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (2.3.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n","Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from fairscale) (2.0.2)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf) (4.9.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (4.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.1.31)\n"]}]},{"cell_type":"code","source":["\n","\n","import os\n","import torch\n","import numpy as np\n","from PIL import Image\n","import torchaudio\n","from transformers import CLIPProcessor, CLIPModel, Wav2Vec2Processor, Wav2Vec2Model\n","from lavis.models import load_model_and_preprocess\n","import google.generativeai as genai\n","from google.colab import files\n","import faiss\n","\n","# ================================\n","# 🔐 CONFIGURE GEMINI\n","# ================================\n","GEMINI_API_KEY = \"\"  # Replace this\n","genai.configure(api_key=GEMINI_API_KEY)\n","gemini = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n","\n","# ================================\n","# 🧠 LOAD ENCODERS\n","# ================================\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# CLIP for text & image\n","clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n","clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","\n","# Wav2Vec2 for audio\n","audio_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","audio_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n","\n","# BLIP for image captioning\n","blip_model, vis_processors, _ = load_model_and_preprocess(\n","    name=\"blip_caption\", model_type=\"base_coco\", is_eval=True, device=device\n",")\n","\n","# ================================\n","# 🔧 ENCODING FUNCTIONS\n","# ================================\n","def encode_text(text):\n","    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n","    with torch.no_grad():\n","        features = clip_model.get_text_features(**inputs)\n","    return features[0].cpu().numpy()\n","\n","def encode_image(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n","    with torch.no_grad():\n","        features = clip_model.get_image_features(**inputs)\n","    return features[0].cpu().numpy()\n","\n","def encode_audio(audio_path):\n","    waveform, sr = torchaudio.load(audio_path)\n","    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)(waveform)\n","    input_values = audio_processor(waveform.squeeze(0), return_tensors=\"pt\", sampling_rate=16000).input_values.to(device)\n","    with torch.no_grad():\n","        features = audio_model(input_values).last_hidden_state.mean(dim=1)\n","    return features[0].cpu().numpy()\n","\n","def get_image_caption(image_path):\n","    raw_image = Image.open(image_path).convert(\"RGB\")\n","    image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n","    caption = blip_model.generate({\"image\": image})[0]\n","    return caption\n","\n","# ================================\n","# 📚 KNOWLEDGE BASE + FAISS\n","# ================================\n","knowledge_base = [\n","    \"A spectrogram shows frequency over time and intensity.\",\n","    \"Bar charts are ideal for comparing quantities across categories.\",\n","    \"Waveforms represent audio amplitude over time.\",\n","    \"Histograms are great for visualizing distributions of numeric data.\"\n","]\n","\n","kb_embeddings = np.array([encode_text(doc) for doc in knowledge_base]).astype(\"float32\")\n","dimension = kb_embeddings.shape[1]\n","index = faiss.IndexFlatL2(dimension)\n","index.add(kb_embeddings)\n","\n","def retrieve_relevant_docs(query_embedding, top_k=2):\n","    query_embedding = np.expand_dims(query_embedding.astype(\"float32\"), axis=0)\n","    distances, indices = index.search(query_embedding, top_k)\n","    return [knowledge_base[i] for i in indices[0]]\n","\n","# ================================\n","# 🎯 GEMINI RAG CALL\n","# ================================\n","def generate_with_gemini_1_5(image_path, user_query, retrieved_docs, image_caption=None):\n","    vision_data = Image.open(image_path).convert(\"RGB\") if image_path else None\n","    prompt = f\"\"\"\n","You are a helpful assistant. Answer the user query based on retrieved context and input modality.\n","\n","User Question:\n","{user_query}\n","\n","Retrieved Context:\n","{retrieved_docs}\n","\n","Image Description (if any):\n","{image_caption if image_caption else \"No image uploaded.\"}\n","\"\"\"\n","\n","    parts = [prompt]\n","    if vision_data:\n","        parts.append(vision_data)\n","\n","    response = gemini.generate_content(parts)\n","    return response.text\n","\n","# ================================\n","# 🚀 MAIN MULTIMODAL RAG\n","# ================================\n","print(\"What input modalities will you use?\")\n","print(\"1 - Text\\n2 - Image\\n3 - Audio\\n(You can combine, e.g., 1 2 3)\")\n","selected = input(\"Enter choice(s): \").split()\n","\n","use_text = '1' in selected\n","use_image = '2' in selected\n","use_audio = '3' in selected\n","\n","vectors = []\n","image_path, audio_path, text_query = None, None, \"What is shown or heard here?\"\n","caption = \"\"\n","\n","# 📁 Upload image\n","if use_image:\n","    print(\"📁 Upload an image:\")\n","    uploaded = files.upload()\n","    image_path = list(uploaded.keys())[0]\n","    caption = get_image_caption(image_path)\n","    vectors.append(encode_image(image_path))\n","\n","# 🎧 Upload audio\n","if use_audio:\n","    print(\"🎧 Upload audio file (.wav):\")\n","    uploaded = files.upload()\n","    audio_path = list(uploaded.keys())[0]\n","    vectors.append(encode_audio(audio_path))\n","\n","# ✍️ Text input\n","if use_text:\n","    text_query = input(\"📝 Enter your text query: \")\n","    vectors.append(encode_text(text_query))\n","\n","if not vectors:\n","    print(\"❌ No inputs provided. Please select at least one modality.\")\n","else:\n","    # 🔍 Retrieve context and answer\n","    query_vec = np.mean(np.stack(vectors), axis=0)\n","    retrieved = retrieve_relevant_docs(query_vec)\n","\n","    print(\"\\n📚 Retrieved Docs:\")\n","    for doc in retrieved:\n","        print(\"•\", doc)\n","\n","    final_answer = generate_with_gemini_1_5(\n","        image_path=image_path if use_image else None,\n","        user_query=text_query,\n","        retrieved_docs=\" \".join(retrieved),\n","        image_caption=caption if use_image else None\n","    )\n","\n","    print(\"\\n🤖 Gemini 1.5 Answer:\\n\", final_answer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"id":"e5JpNe8IM3Y4","executionInfo":{"status":"error","timestamp":1743828013051,"user_tz":-330,"elapsed":65,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"d1ac31d5-4eb8-4f43-fc87-bd9cf8270f43"},"execution_count":6,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'lavis'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-43bcc9dd50c9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCLIPProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIPModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWav2Vec2Processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWav2Vec2Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlavis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model_and_preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerativeai\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lavis'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["import os\n","import torch\n","import numpy as np\n","from PIL import Image\n","import torchaudio\n","from transformers import (\n","    CLIPProcessor, CLIPModel,\n","    Wav2Vec2Processor, Wav2Vec2Model,\n","    BlipProcessor, BlipForConditionalGeneration\n",")\n","import google.generativeai as genai\n","from google.colab import files\n","import faiss\n","\n","# ================================\n","# 🔐 GEMINI API KEY\n","# ================================\n","GEMINI_API_KEY = \"AIzaSyAJtk_kLPx7s5YWcrfbMBVuaKxqmswTSU0\"  # ⬅️ Replace with your actual API key\n","genai.configure(api_key=GEMINI_API_KEY)\n","gemini = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n","\n","# ================================\n","# 🧠 LOAD MODELS\n","# ================================\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# CLIP for text & image\n","clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n","clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","\n","# Wav2Vec2 for audio\n","audio_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","audio_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n","\n","# BLIP for image captioning\n","blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n","\n","# ================================\n","# 🔧 ENCODING FUNCTIONS\n","# ================================\n","def encode_text(text):\n","    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n","    with torch.no_grad():\n","        features = clip_model.get_text_features(**inputs)\n","    return features[0].cpu().numpy()\n","\n","def encode_image(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n","    with torch.no_grad():\n","        features = clip_model.get_image_features(**inputs)\n","    return features[0].cpu().numpy()\n","\n","def encode_audio(audio_path):\n","    waveform, sr = torchaudio.load(audio_path)\n","    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)(waveform)\n","    input_values = audio_processor(waveform.squeeze(0), return_tensors=\"pt\", sampling_rate=16000).input_values.to(device)\n","    with torch.no_grad():\n","        features = audio_model(input_values).last_hidden_state.mean(dim=1)\n","    return features[0].cpu().numpy()\n","\n","def get_image_caption(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n","    output = blip_model.generate(**inputs, max_new_tokens=30)\n","    caption = blip_processor.decode(output[0], skip_special_tokens=True)\n","    return caption\n","\n","# ================================\n","# 📚 KNOWLEDGE BASE + FAISS\n","# ================================\n","knowledge_base = [\n","    \"A spectrogram shows frequency over time and intensity.\",\n","    \"Bar charts are ideal for comparing quantities across categories.\",\n","    \"Waveforms represent audio amplitude over time.\",\n","    \"Histograms are great for visualizing distributions of numeric data.\"\n","]\n","\n","kb_embeddings = np.array([encode_text(doc) for doc in knowledge_base]).astype(\"float32\")\n","dimension = kb_embeddings.shape[1]\n","index = faiss.IndexFlatL2(dimension)\n","index.add(kb_embeddings)\n","\n","def retrieve_relevant_docs(query_embedding, top_k=2):\n","    query_embedding = np.expand_dims(query_embedding.astype(\"float32\"), axis=0)\n","    distances, indices = index.search(query_embedding, top_k)\n","    return [knowledge_base[i] for i in indices[0]]\n","\n","# ================================\n","# 🎯 GEMINI 1.5 GENERATION\n","# ================================\n","def generate_with_gemini_1_5(image_path, user_query, retrieved_docs, image_caption=None):\n","    vision_data = Image.open(image_path).convert(\"RGB\") if image_path else None\n","\n","\n","    prompt = f\"\"\"\n","You are a helpful assistant. Answer the user query based on retrieved context and input modality.\n","\n","User Question:\n","{user_query}\n","\n","Retrieved Context:\n","{retrieved_docs}\n","\n","Image Description (if any):\n","{image_caption if image_caption else \"No image uploaded.\"}\n","\"\"\"\n","\n","    parts = [prompt]\n","    if vision_data:\n","        parts.append(vision_data)\n","\n","    response = gemini.generate_content(parts)\n","    return response.text\n","\n","# ================================\n","# 🚀 MULTIMODAL RAG FLOW\n","# ================================\n","print(\"What input modalities will you use?\")\n","print(\"1 - Text\\n2 - Image\\n3 - Audio\\n(You can combine, e.g., 1 2 3)\")\n","selected = input(\"Enter choice(s): \").split()\n","\n","use_text = '1' in selected\n","use_image = '2' in selected\n","use_audio = '3' in selected\n","\n","vectors = []\n","image_path, audio_path, text_query = None, None, \"What is shown or heard here?\"\n","caption = \"\"\n","\n","# 📁 Upload image\n","if use_image:\n","    print(\"📁 Upload an image file:\")\n","    uploaded = files.upload()\n","    image_path = list(uploaded.keys())[0]\n","    caption = get_image_caption(image_path)\n","    vectors.append(encode_image(image_path))\n","\n","# 🎧 Upload audio\n","if use_audio:\n","    print(\"🎧 Upload audio file (.wav):\")\n","    uploaded = files.upload()\n","    audio_path = list(uploaded.keys())[0]\n","    vectors.append(encode_audio(audio_path))\n","\n","# ✍️ Text input\n","if use_text:\n","    text_query = input(\"📝 Enter your text query: \")\n","    vectors.append(encode_text(text_query))\n","\n","from IPython.display import display\n","import ipywidgets as widgets\n","\n","if not vectors:\n","    print(\"❌ No inputs provided. Please select at least one modality.\")\n","else:\n","    # Display text box for user query\n","    question_box = widgets.Text(\n","        placeholder='Ask a question about the uploaded file...',\n","        description='Your Q:',\n","        layout=widgets.Layout(width='100%'),\n","        style={'description_width': 'initial'}\n","    )\n","    display(question_box)\n","\n","    output_box = widgets.Output()\n","    display(output_box)\n","\n","    def on_submit(change):\n","        with output_box:\n","            output_box.clear_output()\n","            user_question = question_box.value.strip()\n","            if user_question:\n","                # You can still use retrieval if desired (optional)\n","                # query_vec = encode_text(user_question)\n","                # retrieved = retrieve_relevant_docs(query_vec)\n","\n","                answer = generate_with_gemini_1_5(\n","                    image_path=image_path if use_image else None,\n","                    user_query=user_question,\n","                    retrieved_docs=\"\",  # Retrieval disabled here\n","                    image_caption=caption if use_image else None\n","                )\n","                print(\"🤖 Gemini 1.5 Answer:\\n\", answer)\n","            else:\n","                print(\"⚠️ Please ask a question.\")\n","\n","    question_box.on_submit(on_submit)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"id":"_CjqCBiIO7-w","executionInfo":{"status":"error","timestamp":1743829195585,"user_tz":-330,"elapsed":157234,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"2fe0bb5d-24dd-4338-e74f-723ee7e83d5c"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["What input modalities will you use?\n","1 - Text\n","2 - Image\n","3 - Audio\n","(You can combine, e.g., 1 2 3)\n","Enter choice(s): 1\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-97657f2684f5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;31m# ✍️ Text input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mtext_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"📝 Enter your text query: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import numpy as np\n","from PIL import Image\n","import torchaudio\n","from transformers import (\n","    CLIPProcessor, CLIPModel,\n","    Wav2Vec2Processor, Wav2Vec2Model,\n","    BlipProcessor, BlipForConditionalGeneration\n",")\n","import google.generativeai as genai\n","from google.colab import files\n","import faiss\n","\n","# ================================\n","# 🔐 GEMINI API KEY\n","# ================================\n","GEMINI_API_KEY = \"AIzaSyAJtk_kLPx7s5YWcrfbMBVuaKxqmswTSU0\"  # ⬅️ Replace with your actual API key\n","genai.configure(api_key=GEMINI_API_KEY)\n","gemini = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n","\n","# ================================\n","# 🧠 LOAD MODELS\n","# ================================\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# CLIP for text & image\n","clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n","clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","\n","# Wav2Vec2 for audio\n","audio_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","audio_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n","\n","# BLIP for image captioning\n","blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n","\n","# ================================\n","# 🔧 ENCODING FUNCTIONS\n","# ================================\n","def encode_text(text):\n","    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n","    with torch.no_grad():\n","        features = clip_model.get_text_features(**inputs)\n","    return features[0].cpu().numpy()\n","\n","def encode_image(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n","    with torch.no_grad():\n","        features = clip_model.get_image_features(**inputs)\n","    return features[0].cpu().numpy()\n","\n","def encode_audio(audio_path):\n","    waveform, sr = torchaudio.load(audio_path)\n","    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)(waveform)\n","    input_values = audio_processor(waveform.squeeze(0), return_tensors=\"pt\", sampling_rate=16000).input_values.to(device)\n","    with torch.no_grad():\n","        features = audio_model(input_values).last_hidden_state.mean(dim=1)\n","    return features[0].cpu().numpy()\n","\n","def get_image_caption(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n","    output = blip_model.generate(**inputs, max_new_tokens=30)\n","    caption = blip_processor.decode(output[0], skip_special_tokens=True)\n","    return caption\n","\n","# ================================\n","# 🎯 GEMINI 1.5 GENERATION\n","# ================================\n","def generate_with_gemini_1_5(user_query, context=None, image_path=None, image_caption=None):\n","    vision_data = Image.open(image_path).convert(\"RGB\") if image_path else None\n","\n","    prompt = f\"\"\"\n","You are a helpful assistant. Answer the user's question based on the context provided.\n","\n","User Question:\n","{user_query}\n","\n","Context:\n","{context if context else \"No additional context.\"}\n","\n","Image Description:\n","{image_caption if image_caption else \"No image uploaded.\"}\n","\"\"\"\n","\n","    parts = [prompt]\n","    if vision_data:\n","        parts.append(vision_data)\n","\n","    response = gemini.generate_content(parts)\n","    return response.text\n","\n","# ================================\n","# 🚀 INTERACTIVE LOOP\n","# ================================\n","while True:\n","    print(\"\\nWhat input modality will you use?\")\n","    print(\"1 - Text\\n2 - Image\\n3 - Audio\\n4 - Exit\")\n","    selected = input(\"Enter choice: \").strip()\n","\n","    if selected == '4':\n","        print(\"👋 Exiting...\")\n","        break\n","\n","    text_context = \"\"\n","    image_path = None\n","    audio_path = None\n","    image_caption = \"\"\n","\n","    if selected == '1':\n","        print(\"✍️ Enter a paragraph of text:\")\n","        text_context = input()\n","\n","    elif selected == '2':\n","        print(\"📁 Upload an image file:\")\n","        uploaded = files.upload()\n","        image_path = list(uploaded.keys())[0]\n","        image_caption = get_image_caption(image_path)\n","\n","    elif selected == '3':\n","        print(\"🎧 Upload an audio file (.wav):\")\n","        uploaded = files.upload()\n","        audio_path = list(uploaded.keys())[0]\n","\n","    else:\n","        print(\"❌ Invalid option. Please try again.\")\n","        continue\n","\n","    # Ask questions iteratively\n","    while True:\n","        user_question = input(\"\\n❓ Ask a question (or type 'exit' to go back): \").strip()\n","        if user_question.lower() == 'exit':\n","            break\n","\n","        answer = generate_with_gemini_1_5(\n","            user_query=user_question,\n","            context=text_context if selected == '1' else None,\n","            image_path=image_path if selected == '2' else None,\n","            image_caption=image_caption if selected == '2' else None\n","        )\n","        print(\"🤖 Gemini 1.5 Answer:\\n\", answer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2sxE1STaTRbm","executionInfo":{"status":"ok","timestamp":1743830192587,"user_tz":-330,"elapsed":537126,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"7cb008a0-3fee-4c25-d7c9-7d64fa2f0576"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","What input modality will you use?\n","1 - Text\n","2 - Image\n","3 - Audio\n","4 - Exit\n","Enter choice: 1\n","✍️ Enter a paragraph of text:\n","Coronavirus (COVID-19) is a highly infectious disease caused by the novel coronavirus SARS-CoV-2. It first emerged in Wuhan, China, in late 2019 and rapidly spread across the globe, leading to a worldwide pandemic. The virus primarily spreads through respiratory droplets and aerosols, and can cause symptoms ranging from mild respiratory issues to severe pneumonia and death. Preventive measures such as vaccination, mask-wearing, social distancing, and good hand hygiene have been key in controlling its spread. Ongoing research focuses on understanding variants and improving treatment strategies.\n","\n","❓ Ask a question (or type 'exit' to go back): where corona virus emerged first?\n","🤖 Gemini 1.5 Answer:\n"," Wuhan, China.\n","\n","\n","❓ Ask a question (or type 'exit' to go back): exit\n","\n","What input modality will you use?\n","1 - Text\n","2 - Image\n","3 - Audio\n","4 - Exit\n","Enter choice: 2\n","📁 Upload an image file:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-7bf3bc76-aabb-4fc6-9a00-881976e96771\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-7bf3bc76-aabb-4fc6-9a00-881976e96771\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving ying.jfif to ying (5).jfif\n","\n","❓ Ask a question (or type 'exit' to go back): what is the origin of this symbol?\n","🤖 Gemini 1.5 Answer:\n"," That's the yin and yang symbol, originating from ancient Chinese philosophy and cosmology.  It represents the interconnectedness and interdependence of opposite forces in the universe, such as light and dark, male and female, and action and inaction.  It's a key concept in Taoism.\n","\n","\n","❓ Ask a question (or type 'exit' to go back): exit\n","\n","What input modality will you use?\n","1 - Text\n","2 - Image\n","3 - Audio\n","4 - Exit\n","Enter choice: 3\n","🎧 Upload an audio file (.wav):\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-7b7f7306-fbff-4f4a-8594-2390ba983ae0\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-7b7f7306-fbff-4f4a-8594-2390ba983ae0\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving sample-6s.mp3 to sample-6s.mp3\n","\n","❓ Ask a question (or type 'exit' to go back): what is the name of this music?\n","🤖 Gemini 1.5 Answer:\n"," I do not have enough information to answer what music is playing.  I have no context or audio to work with.\n","\n","\n","❓ Ask a question (or type 'exit' to go back): exit\n","\n","What input modality will you use?\n","1 - Text\n","2 - Image\n","3 - Audio\n","4 - Exit\n","Enter choice: 3\n","🎧 Upload an audio file (.wav):\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-bb2b2855-1475-4f7c-9ad0-564455f0ee61\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-bb2b2855-1475-4f7c-9ad0-564455f0ee61\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving health 011509 wisdom teeth_0.mp3 to health 011509 wisdom teeth_0.mp3\n","\n","❓ Ask a question (or type 'exit' to go back): what is wisdom teeth?\n","🤖 Gemini 1.5 Answer:\n"," Wisdom teeth are the third molars, typically the last teeth to erupt in the mouth, usually between the ages of 17 and 25.  They are located in the very back of the mouth, both top and bottom.\n","\n","\n","❓ Ask a question (or type 'exit' to go back): so you have answered this question based on the audio i provided or not?\n","🤖 Gemini 1.5 Answer:\n"," I have no access to audio or any files you might have provided outside of this current text interaction. So, the answer I've given is not based on any audio you've previously shared.\n","\n","\n","❓ Ask a question (or type 'exit' to go back): exit\n","\n","What input modality will you use?\n","1 - Text\n","2 - Image\n","3 - Audio\n","4 - Exit\n","Enter choice: exit\n","❌ Invalid option. Please try again.\n","\n","What input modality will you use?\n","1 - Text\n","2 - Image\n","3 - Audio\n","4 - Exit\n","Enter choice: 4\n","👋 Exiting...\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import numpy as np\n","from PIL import Image\n","import torchaudio\n","from transformers import (\n","    CLIPProcessor, CLIPModel,\n","    Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2Tokenizer,\n","    BlipProcessor, BlipForConditionalGeneration\n",")\n","import google.generativeai as genai\n","from google.colab import files\n","import faiss\n","\n","# ================================\n","# 🔐 GEMINI API KEY\n","# ================================\n","GEMINI_API_KEY = \"AIzaSyAJtk_kLPx7s5YWcrfbMBVuaKxqmswTSU0\"  # ⬅️ Replace with your actual API key\n","genai.configure(api_key=GEMINI_API_KEY)\n","gemini = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n","\n","# ================================\n","# 🧠 LOAD MODELS\n","# ================================\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# CLIP for text & image\n","clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n","clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","\n","# Wav2Vec2 for audio transcription\n","audio_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","audio_tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","audio_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n","\n","# BLIP for image captioning\n","blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n","\n","# ================================\n","# 🔧 ENCODING FUNCTIONS\n","# ================================\n","def encode_text(text):\n","    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n","    with torch.no_grad():\n","        features = clip_model.get_text_features(**inputs)\n","    return features[0].cpu().numpy()\n","\n","def encode_image(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n","    with torch.no_grad():\n","        features = clip_model.get_image_features(**inputs)\n","    return features[0].cpu().numpy()\n","\n","def encode_audio(audio_path):\n","    waveform, sr = torchaudio.load(audio_path)\n","    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)(waveform)\n","    input_values = audio_processor(waveform.squeeze(0), return_tensors=\"pt\", sampling_rate=16000).input_values.to(device)\n","    with torch.no_grad():\n","        logits = audio_model(input_values).logits\n","        predicted_ids = torch.argmax(logits, dim=-1)\n","        transcription = audio_tokenizer.batch_decode(predicted_ids)[0].lower()\n","    return transcription\n","\n","def get_image_caption(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n","    output = blip_model.generate(**inputs, max_new_tokens=30)\n","    caption = blip_processor.decode(output[0], skip_special_tokens=True)\n","    return caption\n","\n","# ================================\n","# 🎯 GEMINI 1.5 GENERATION\n","# ================================\n","def generate_with_gemini_1_5(user_query, context=None, image_path=None, image_caption=None):\n","    vision_data = Image.open(image_path).convert(\"RGB\") if image_path else None\n","\n","    prompt = f\"\"\"\n","You are a helpful assistant. Answer the user's question based on the context provided.\n","\n","User Question:\n","{user_query}\n","\n","Context:\n","{context if context else \"No additional context.\"}\n","\n","Image Description:\n","{image_caption if image_caption else \"No image uploaded.\"}\n","\"\"\"\n","\n","    parts = [prompt]\n","    if vision_data:\n","        parts.append(vision_data)\n","\n","    response = gemini.generate_content(parts)\n","    return response.text\n","\n","# ================================\n","# 🚀 INTERACTIVE LOOP\n","# ================================\n","while True:\n","    print(\"\\nWhat input modality will you use?\")\n","    print(\"1 - Text\\n2 - Image\\n3 - Audio\\n4 - Exit\")\n","    selected = input(\"Enter choice: \").strip()\n","\n","    if selected == '4':\n","        print(\"👋 Exiting...\")\n","        break\n","\n","    text_context = \"\"\n","    image_path = None\n","    audio_path = None\n","    image_caption = \"\"\n","\n","    if selected == '1':\n","        print(\"✍️ Enter a paragraph of text:\")\n","        text_context = input()\n","\n","    elif selected == '2':\n","        print(\"📁 Upload an image file:\")\n","        uploaded = files.upload()\n","        image_path = list(uploaded.keys())[0]\n","        image_caption = get_image_caption(image_path)\n","\n","    elif selected == '3':\n","        print(\"🎧 Upload an audio file (.wav):\")\n","        uploaded = files.upload()\n","        audio_path = list(uploaded.keys())[0]\n","        text_context = encode_audio(audio_path)  # Transcribe audio to text\n","        print(\"📝 Transcription of audio:\")\n","        print(text_context)\n","\n","    else:\n","        print(\"❌ Invalid option. Please try again.\")\n","        continue\n","\n","    # Ask questions iteratively\n","    while True:\n","        user_question = input(\"\\n❓ Ask a question (or type 'exit' to go back): \").strip()\n","        if user_question.lower() == 'exit':\n","            break\n","\n","        answer = generate_with_gemini_1_5(\n","            user_query=user_question,\n","            context=text_context if selected in ['1', '3'] else None,\n","            image_path=image_path if selected == '2' else None,\n","            image_caption=image_caption if selected == '2' else None\n","        )\n","        print(\"🤖 Gemini 1.5 Answer:\\n\", answer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":525},"id":"FVgo5xBtYxYs","executionInfo":{"status":"ok","timestamp":1743830692218,"user_tz":-330,"elapsed":208675,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"777ed544-66a2-4e4e-cb44-6a96fd56894d"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n","The class this function is called from is 'Wav2Vec2Tokenizer'.\n","Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","What input modality will you use?\n","1 - Text\n","2 - Image\n","3 - Audio\n","4 - Exit\n","Enter choice: 3\n","🎧 Upload an audio file (.wav):\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-0b20563e-471f-465c-8a5b-adb387efbd2e\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-0b20563e-471f-465c-8a5b-adb387efbd2e\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving health 011509 wisdom teeth_0.mp3 to health 011509 wisdom teeth_0 (2).mp3\n","📝 Transcription of audio:\n","this is the veoa special english health report wisdom teeth are normally the last teeth to appear in the mouth it usually happens when people are older and wiser that is when there in their late teanage years or early twenties wisdom teeth are molers or chewing teeth at the back of the mouth the third set of molers if you have them are your wisdom teeth they can grow into place normally and never cause a problem but often there is not enough room for them in the mouth they might crowd the other teeth sometimes they even push through the gums sideways and impacted wisdom teeth is one that fails to completely rise through the gums the term is erupt wisdom teeth that only partly erupt can leave space for bacteria to enter around the tooth infection is a risk in these cases experts say people should have their mouths examined between the ages of sixteen and twentyt for placement of their wisdom teeth exprats cand show wisdom teeth below the gums those that are not well alined and become impacted are often removed the american dental association says removal is generally advised when wisdom teeth only partly break through the gums removal is also advised if there is a chance that poorly alined wisdom teeth will damage other teeth and removal is called for in cases where fluid collects around a wisdom tooth that is partly or fully below the gum but hy do we have wisdom teeth if we often need to get them removed one theory has to do with our diets scientists say the diet of prehistoric humans probably required more chewing teeth life was probably a little rougher on the teeth back then too so it was good to have extras the removal of wisdom teeth is performed by oral surgeons they say if removal is advised the best time to  de it is befor the teeth cause any problems or pain the american association of oral and maxilophacial surgeons says young adults are the best candidaths for wisdom teeth removal the group says older pations may be at greater risk for diseasth in the tissue surrounding the molers patients can have general anesthesia during the operation or they might ches to have a local pain killer and remain awake it may depend on the condition of the wisdom teeth and the number to be removed after surgery there can be swelling of the gums and faith and some pain both can be treated with cold raps and medication and thats the veoa special english health report written by caty weaver i'm faithulapitis\n","\n","❓ Ask a question (or type 'exit' to go back): exit\n","\n","What input modality will you use?\n","1 - Text\n","2 - Image\n","3 - Audio\n","4 - Exit\n","Enter choice: 4\n","👋 Exiting...\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import numpy as np\n","from PIL import Image\n","import torchaudio\n","from transformers import (\n","    CLIPProcessor, CLIPModel,\n","    Wav2Vec2Processor, Wav2Vec2ForCTC,\n","    BlipProcessor, BlipForConditionalGeneration\n",")\n","import google.generativeai as genai\n","from google.colab import files\n","import faiss\n","\n","# Enable MP3 support\n","torchaudio.set_audio_backend(\"sox_io\")\n","\n","# ================================\n","# 🔐 GEMINI API KEY\n","# ================================\n","GEMINI_API_KEY = \"AIzaSyAJtk_kLPx7s5YWcrfbMBVuaKxqmswTSU0\"  # ⮆️ Replace with your actual API key\n","genai.configure(api_key=GEMINI_API_KEY)\n","gemini = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n","\n","# ================================\n","# 🧠 LOAD MODELS\n","# ================================\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# CLIP for text & image\n","clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n","clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","\n","# Wav2Vec2 for audio transcription\n","audio_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","audio_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n","\n","# BLIP for image captioning\n","blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n","\n","# ================================\n","# 🔧 ENCODING FUNCTIONS\n","# ================================\n","def encode_text(text):\n","    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n","    with torch.no_grad():\n","        features = clip_model.get_text_features(**inputs)\n","    return features[0].cpu().numpy()\n","\n","def encode_image(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n","    with torch.no_grad():\n","        features = clip_model.get_image_features(**inputs)\n","    return features[0].cpu().numpy()\n","\n","def encode_audio(audio_path):\n","    waveform, sr = torchaudio.load(audio_path)\n","    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)(waveform)\n","    input_values = audio_processor(waveform.squeeze(0), return_tensors=\"pt\", sampling_rate=16000).input_values.to(device)\n","    with torch.no_grad():\n","        logits = audio_model(input_values).logits\n","        predicted_ids = torch.argmax(logits, dim=-1)\n","        transcription = audio_processor.batch_decode(predicted_ids)[0].lower()\n","    return transcription\n","\n","def get_image_caption(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n","    output = blip_model.generate(**inputs, max_new_tokens=30)\n","    caption = blip_processor.decode(output[0], skip_special_tokens=True)\n","    return caption\n","\n","# ================================\n","# 🎯 GEMINI 1.5 GENERATION\n","# ================================\n","def generate_with_gemini_1_5(user_query, context=None, image_path=None, image_caption=None):\n","    vision_data = Image.open(image_path).convert(\"RGB\") if image_path else None\n","\n","    prompt = f\"\"\"\n","You are a helpful assistant. Answer the user's question based on the context provided.\n","\n","User Question:\n","{user_query}\n","\n","Context:\n","{context if context else \"No additional context.\"}\n","\n","Image Description:\n","{image_caption if image_caption else \"No image uploaded.\"}\n","\"\"\"\n","\n","    parts = [prompt]\n","    if vision_data:\n","        parts.append(vision_data)\n","\n","    response = gemini.generate_content(parts)\n","    return response.text\n","\n","# ================================\n","# 🚀 INTERACTIVE LOOP\n","# ================================\n","while True:\n","    print(\"\\nWhat input modality will you use?\")\n","    print(\"1 - Text\\n2 - Image\\n3 - Audio\\n4 - Exit\")\n","    selected = input(\"Enter choice: \").strip()\n","\n","    if selected == '4':\n","        print(\"👋 Exiting...\")\n","        break\n","\n","    text_context = \"\"\n","    image_path = None\n","    audio_path = None\n","    image_caption = \"\"\n","\n","    if selected == '1':\n","        print(\"✍️ Enter a paragraph of text:\")\n","        text_context = input()\n","\n","    elif selected == '2':\n","        print(\"📁 Upload an image file:\")\n","        uploaded = files.upload()\n","        image_path = list(uploaded.keys())[0]\n","        image_caption = get_image_caption(image_path)\n","\n","    elif selected == '3':\n","        print(\"🎷 Upload an audio file (.mp3 or .wav):\")\n","        uploaded = files.upload()\n","        audio_path = list(uploaded.keys())[0]\n","        text_context = encode_audio(audio_path)\n","        print(\"📝 Transcription of audio:\")\n","        print(text_context)\n","\n","    else:\n","        print(\"❌ Invalid option. Please try again.\")\n","        continue\n","\n","    # Ask questions iteratively\n","    while True:\n","        user_question = input(\"\\n❓ Ask a question (or type 'exit' to go back): \").strip()\n","        if user_question.lower() == 'exit':\n","            break\n","\n","        answer = generate_with_gemini_1_5(\n","            user_query=user_question,\n","            context=text_context if selected in ['1', '3'] else None,\n","            image_path=image_path if selected == '2' else None,\n","            image_caption=image_caption if selected == '2' else None\n","        )\n","        print(\"🧠 Gemini 1.5 Answer:\\n\", answer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":560},"id":"7xuXWvmNZYFT","executionInfo":{"status":"ok","timestamp":1743830878913,"user_tz":-330,"elapsed":71140,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"91b57c06-883e-453a-b22b-51c1bfd0dde8"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-24-c65c748609ce>:16: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n","  torchaudio.set_audio_backend(\"sox_io\")\n","Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","What input modality will you use?\n","1 - Text\n","2 - Image\n","3 - Audio\n","4 - Exit\n","Enter choice: 2\n","📁 Upload an image file:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-eba6b901-bc45-41b9-961c-46737311e376\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-eba6b901-bc45-41b9-961c-46737311e376\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving ying.jfif to ying (6).jfif\n","\n","❓ Ask a question (or type 'exit' to go back): what does yin mean\n","🧠 Gemini 1.5 Answer:\n"," Yin, represented by the dark swirl in the symbol, is one half of the Taoist concept of duality.  It is associated with the feminine, passive, dark, cold, and yielding aspects of existence.  It complements and is interconnected with yang, the light swirl.  Together, yin and yang represent the interplay of opposing forces that make up all aspects of the universe and life.\n","\n","\n","❓ Ask a question (or type 'exit' to go back): exit\n","\n","What input modality will you use?\n","1 - Text\n","2 - Image\n","3 - Audio\n","4 - Exit\n","Enter choice: 4\n","👋 Exiting...\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"2vYrJuCqaML7"},"execution_count":null,"outputs":[]}]}
